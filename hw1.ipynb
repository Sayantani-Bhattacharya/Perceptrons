{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vTszHCDem4yJ"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "################################################################################\n",
    "# Some simple plotting utilities\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def plot_data(data: np.ndarray,\n",
    "              labels: np.ndarray,\n",
    "              ax: matplotlib.axes.Axes = None):\n",
    "    \"\"\"\n",
    "    A helper function to plot our data sets\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    data      A numpy array of 2 columns (dimensions) and 2*examples_per_class rows\n",
    "\n",
    "    labels    A numpy vector with 2*examples_per_class, with a +1 or -1 in each\n",
    "              element. The jth element is the label of the jth example\n",
    "\n",
    "    ax        An optional matplotlib axis object to plot to\n",
    "    \"\"\"\n",
    "\n",
    "    # require shape (n, 2)\n",
    "    assert data.ndim == 2\n",
    "    assert data.shape[-1] == 2\n",
    "\n",
    "    if type(data) == torch.Tensor:\n",
    "        data = data.numpy()\n",
    "\n",
    "    # plot the data\n",
    "    pos_idx = np.where(labels == 1)\n",
    "    neg_idx = np.where(labels == -1)\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt\n",
    "    ax.plot(\n",
    "        data.T[0, pos_idx],\n",
    "        data.T[1, pos_idx],\n",
    "        'r^',\n",
    "        label='positive'\n",
    "    )\n",
    "    ax.plot(\n",
    "        data.T[0, neg_idx],\n",
    "        data.T[1, neg_idx],\n",
    "        'bo',\n",
    "        label='negative'\n",
    "    )\n",
    "    ax.axis('equal')\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc=\"upper right\")\n",
    "\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_decision_surface(model=None,\n",
    "                          axis_limits=(-5, 5, -5, 5),\n",
    "                          ax: matplotlib.axes.Axes = None\n",
    "                          ):\n",
    "    \"\"\"\n",
    "    Creates a grid of points, measures what a model would label each\n",
    "    point as, and uses this data to draw a region for class +1 and a region for\n",
    "    class -1.\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model       A callable model that can take 2-d real-valued input and produce\n",
    "                a +1 or -1 label for each data point.\n",
    "\n",
    "    axis_limits An array-like object with 4 floats [lowest_horizontal, highest_horizontal,\n",
    "                lowest_vertical, highest_vertical]. This sets the limits over which\n",
    "                the decision surface will be caluclated and plotted.\n",
    "\n",
    "    ax          An optional matplotlib axis object to plot to\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    my_contour  a matplotlib.contour.QuadContourSet with the contour\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a grid of points spanning the entire space displayed in the axis.\n",
    "    # This will let us draw the decision boundary later\n",
    "    xx, yy = np.meshgrid(np.arange(axis_limits[0], axis_limits[1], .05),\n",
    "                         np.arange(axis_limits[2], axis_limits[3], .05))\n",
    "    data = np.concatenate([xx.reshape([1, -1]), yy.reshape([1, -1])]).T\n",
    "\n",
    "    # Predict the class of each point in XGrid, using the classifier.\n",
    "    # This shows our regions determined by the classifier\n",
    "    if isinstance(model, nn.Module):\n",
    "        with torch.no_grad():\n",
    "            pl = model(torch.tensor(data).to(dtype=torch.float32))\n",
    "            predicted_labels = np.sign(pl.numpy())\n",
    "    else:\n",
    "        predicted_labels = model(data)\n",
    "\n",
    "    predicted_labels = predicted_labels.reshape(xx.shape)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    if ax is None:\n",
    "        ax = plt\n",
    "\n",
    "    ax.contourf(xx, yy, predicted_labels, cmap=plt.cm.Paired)\n",
    "    ax.axis('equal')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def compute_bounds(features):\n",
    "    min1, max1 = features[:, 0].min()-1, features[:, 0].max()+1\n",
    "    min2, max2 = features[:, 1].min()-1, features[:, 1].max()+1\n",
    "    return (min1, max1, min2, max2)\n",
    "\n",
    "\n",
    "def plot_decision_regions(\n",
    "        features, targets, model,\n",
    "        axis=None, transform=None,\n",
    "        bounds=None,\n",
    "        title='Decision Surface'):\n",
    "    \"\"\"\n",
    "    Slightly different plotting approach than above. Used in backprop demo.\n",
    "\n",
    "    This function produces a single plot containing a scatter plot of the\n",
    "    features, targets, and decision region of the model.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): 2D array containing real-valued inputs.\n",
    "        targets (np.ndarray): 1D array containing binary targets.\n",
    "        model: a learner with .predict() method\n",
    "        axis: the axis on which to plot. If None, create a new plot\n",
    "        title: title of the plot\n",
    "    Returns:\n",
    "        None (plots to the active figure)\n",
    "    \"\"\"\n",
    "\n",
    "    # define bounds of the domain\n",
    "    if bounds is None:\n",
    "        min1, max1, min2, max2 = compute_bounds(features)\n",
    "    else:\n",
    "        min1, max1, min2, max2 = bounds\n",
    "\n",
    "    # define grid for visualizing decision regions\n",
    "    x1grid = np.arange(min1, max1, 0.1)\n",
    "    x2grid = np.arange(min2, max2, 0.1)\n",
    "\n",
    "    xx, yy = np.meshgrid(x1grid, x2grid)\n",
    "\n",
    "    # flatten grid to a vector\n",
    "    r1, r2 = xx.flatten(), yy.flatten()\n",
    "    r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n",
    "\n",
    "    # horizontally stack vectors to create x1,x2 input for the model\n",
    "    grid = np.hstack((r1,r2))\n",
    "\n",
    "    # if we're transforming the features, do that now\n",
    "    #     this allows xx and yy to still be in 2D for the visualization\n",
    "    #     but grid has been transformed so it matches up with the fit model\n",
    "    if transform is not None:\n",
    "        grid = transform(grid)\n",
    "\n",
    "    # generate predictions over grid\n",
    "    yhat = model.predict(grid)\n",
    "\n",
    "    # reshape the predictions back into a grid\n",
    "    zz = yhat.reshape(xx.shape)\n",
    "\n",
    "\n",
    "    if axis is None:\n",
    "        fig, axis = plt.subplots()\n",
    "\n",
    "    # plot the grid of x, y and z values as a surface\n",
    "    binary_cmap = matplotlib.colors.ListedColormap(['#9ce8ff', '#ffc773'])\n",
    "    axis.contourf(xx, yy, zz, cmap=binary_cmap, alpha=0.7)\n",
    "\n",
    "    # plot \"negative\" class:\n",
    "    row_idx_neg = np.where(targets < 0.5)[0]\n",
    "    axis.scatter(\n",
    "        features[row_idx_neg, 0], features[row_idx_neg, 1],\n",
    "        label='negative')\n",
    "\n",
    "    # plot \"positive\" class:\n",
    "    row_idx_pos = np.where(targets > 0.5)[0]\n",
    "    axis.scatter(\n",
    "        features[row_idx_pos, 0], features[row_idx_pos, 1],\n",
    "        label='positive')\n",
    "\n",
    "    axis.set_title(title)\n",
    "    axis.set_xlim(min1, max1)\n",
    "    axis.set_ylim(min2, max2)\n",
    "\n",
    "    axis.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2HEv4Te5XOX0"
   },
   "outputs": [],
   "source": [
    "# Create two datasets\n",
    "\n",
    "def make_multiple_circles(radii, examples_per_ring, std=0.1,):\n",
    "    \"\"\"\n",
    "    A dataset of concentric circles of alternating labels\n",
    "    \"\"\"\n",
    "    assert type(radii) in [list, tuple, np.ndarray]\n",
    "\n",
    "    X = np.zeros([0, 2])\n",
    "    y = np.zeros([0,])\n",
    "\n",
    "    for i, radius in enumerate(radii):\n",
    "        # Rings alternate as labeled -1 or 1\n",
    "        y_ = np.ones([examples_per_ring]) * (-1) ** i\n",
    "        y = np.concatenate([y, y_], axis=0)\n",
    "\n",
    "        # Create a ring of points with radii centered at radius\n",
    "        #   but in any random angle from the origin\n",
    "        r = radius + np.random.normal(0, std, examples_per_ring)\n",
    "        theta = np.random.uniform(0, 2 * np.pi, examples_per_ring)\n",
    "\n",
    "        # Convert (r, theta) coordinates to cartesian\n",
    "        x0 = r * np.sin(theta)\n",
    "        x1 = r * np.cos(theta)\n",
    "        x_ = np.stack([x0, x1], axis=1)\n",
    "        X = np.concatenate([X, x_], axis=0)\n",
    "\n",
    "    return X, y.astype(int)\n",
    "\n",
    "\n",
    "def make_spiral(n):\n",
    "    \"\"\"\n",
    "    Yet another spiral dataset\n",
    "    \"\"\"\n",
    "    y0 = np.zeros(n)\n",
    "    y1 = np.ones(n)\n",
    "    y = np.concatenate([y0, y1], axis=0).astype(int)\n",
    "    z = np.concatenate(2 * [np.linspace(0, 4 * np.pi, n)], axis=0)\n",
    "\n",
    "    x0 = z * np.sin(z + y * np.pi)\n",
    "    x1 = z * np.cos(z + y * np.pi)\n",
    "    X = np.stack([x0, x1], axis=1)\n",
    "\n",
    "    y[y == 0] = -1\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def numpy_to_tensor(lst):\n",
    "    \"\"\"\n",
    "    Convert numpy array to torch tensor\n",
    "    \"\"\"\n",
    "    if type(lst) == np.ndarray:\n",
    "        return torch.tensor(lst).to(dtype=torch.float32)\n",
    "\n",
    "    else:\n",
    "        out = [torch.tensor(item).to(dtype=torch.float32) for item in lst]\n",
    "        return tuple(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "siMil9D6Mq4F"
   },
   "outputs": [],
   "source": [
    "class HW1Net(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes=[], activation=torch.tanh):\n",
    "        \"\"\"\n",
    "        Construct a simple neural network.\n",
    "\n",
    "        Args:\n",
    "         - layer_sizes: a list or tuple of hidden layer sizes. For example,\n",
    "                        if layer_sizes = [4, 4], this will be a network\n",
    "                        with linear layers with dimensions[2, 4], [4, 4], [4, 2]\n",
    "         - activation:  a torch activation function\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        input_dims = (2, ) + tuple(layer_sizes)\n",
    "        output_dims = tuple(layer_sizes) + (2, )\n",
    "        dims = zip(input_dims, output_dims)\n",
    "\n",
    "        self.activation = activation\n",
    "        self.layers = []\n",
    "        for i, (input_dim, output_dim) in enumerate(dims):\n",
    "            # Create and initialize the layer\n",
    "            layer = torch.nn.Linear(input_dim, output_dim)\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "            # Register this layer so Pytorch tracks its parameters\n",
    "            setattr(self, f\"layer{i}\", layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_layers = len(self.layers)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            # Activation on everything but the final layer\n",
    "            if i + 1 < n_layers:\n",
    "                x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "k2Qo3qDgNMFo"
   },
   "outputs": [],
   "source": [
    "def run_one_epoch(model, optimizer, X, y, train=True):\n",
    "\n",
    "    if train:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    output = model(X).squeeze()\n",
    "    binary_targets = torch.where(y > 0, 1, 0)\n",
    "    acc = torch.sum(torch.argmax(output, dim=1) == binary_targets) / y.size(0)\n",
    "    loss = torch.nn.CrossEntropyLoss()(output, binary_targets)\n",
    "\n",
    "    if train:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Detach tells torch to stop tracking a tensor's gradients\n",
    "    return acc.detach(), loss.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "q9YMue18N5Q_"
   },
   "outputs": [],
   "source": [
    "def pretrain_and_train(pretrain_data, data, **kwargs):\n",
    "\n",
    "    # Create the model and set up the optimizer\n",
    "    model = HW1Net(\n",
    "        layer_sizes=kwargs[\"layer_sizes\"],\n",
    "        activation=kwargs[\"activation\"],\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=kwargs[\"learning_rate\"])\n",
    "\n",
    "    # Pretrain the model and save a copy of the pretrained model\n",
    "    X, y = pretrain_data\n",
    "    for _ in range(kwargs[\"n_pretrain_epochs\"]):\n",
    "        run_one_epoch(model, optimizer, X, y)\n",
    "    pretrained_model = copy.deepcopy(model)\n",
    "\n",
    "    # Train the model and track its performance over each epoch\n",
    "    results = defaultdict(list)\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    for i in range(kwargs[\"n_train_epochs\"]):\n",
    "        train_acc, train_loss = run_one_epoch(\n",
    "            model, optimizer, X_train, y_train)\n",
    "\n",
    "        test_acc, test_loss = run_one_epoch(\n",
    "            model, None, X_test, y_test, train=False)\n",
    "\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "\n",
    "    return pretrained_model, model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oeCw7sZoVjwY"
   },
   "outputs": [],
   "source": [
    "def make_prediction_function(model):\n",
    "    \"\"\"\n",
    "    Our network outputs real-valued logits\n",
    "    When we make final predictions, those should be +1 or -1\n",
    "    \"\"\"\n",
    "    def predict(X):\n",
    "        X = numpy_to_tensor(X)\n",
    "        output = model(X).detach()\n",
    "        binary_pred = np.argmax(output, axis=1)\n",
    "        return np.where(binary_pred > 0, 1, -1)\n",
    "\n",
    "    return predict\n",
    "\n",
    "\n",
    "def plot_results(pretrained_model, trained_model,\n",
    "                pretrain_data, data,\n",
    "                results, title):\n",
    "    \"\"\"\n",
    "    Given a (saved) pretrained model, our final trained model,\n",
    "        the pretrained data, train and test data,\n",
    "        and a dictionary of results,\n",
    "\n",
    "    Build the six-panel figure that you'll use to show off your results.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(8, 12),\n",
    "                             constrained_layout=True)\n",
    "\n",
    "    X_pretrain, y_pretrain = pretrain_data\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "\n",
    "    # Top right panel\n",
    "    ax = axes[0, 1]\n",
    "    ax.set_title(\"Pretrained Model on Train Data\")\n",
    "    plot_data(X_train, y_train, ax)\n",
    "    predict = make_prediction_function(pretrained_model)\n",
    "    plot_decision_surface(predict, ax.axis(), ax=ax)\n",
    "\n",
    "    # Top left panel\n",
    "    ax = axes[0, 0]\n",
    "    ax.set_title(\"Pretrained Model on Pretrain Data\")\n",
    "    plot_data(X_pretrain, y_pretrain, ax)\n",
    "    axes[0, 0].set_ylim(axes[0, 1].get_ylim())\n",
    "    axes[0, 0].set_xlim(axes[0, 1].get_xlim())\n",
    "    predict = make_prediction_function(pretrained_model)\n",
    "    plot_decision_surface(predict, ax.axis(), ax=ax)\n",
    "\n",
    "    # Middle left panel\n",
    "    ax = axes[1, 0]\n",
    "    ax.set_title(\"Trained Model on Train Data\")\n",
    "    plot_data(X_train, y_train, ax)\n",
    "    predict = make_prediction_function(trained_model)\n",
    "    plot_decision_surface(predict, ax.axis(), ax=ax)\n",
    "\n",
    "    # Middle right panel\n",
    "    ax = axes[1, 1]\n",
    "    ax.set_title(\"Trained Model on Test Data\")\n",
    "    plot_data(X_test, y_test, ax)\n",
    "    predict = make_prediction_function(trained_model)\n",
    "    plot_decision_surface(predict, ax.axis(), ax=ax)\n",
    "\n",
    "    # Bottom left\n",
    "    ax = axes[2, 0]\n",
    "    ax.set_title(\"Loss per Epoch\")\n",
    "    train_loss = results[\"train_loss\"]\n",
    "    test_loss = results[\"test_loss\"]\n",
    "    n_epochs = len(train_loss)\n",
    "    ax.plot(np.arange(n_epochs), train_loss, c='r', label='Train Loss')\n",
    "    ax.plot(np.arange(n_epochs), test_loss, c='b', label='Test Loss')\n",
    "    ax.legend(loc=\"best\")\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    ax.set_ylim(ymin, 2 * ymax)\n",
    "\n",
    "    # Bottom right\n",
    "    ax = axes[2, 1]\n",
    "    ax.set_title(\"Accuracy per Epoch\")\n",
    "    train_acc = results[\"train_acc\"]\n",
    "    test_acc = results[\"test_acc\"]\n",
    "    n_epochs = len(train_acc)\n",
    "    ax.plot(np.arange(n_epochs), train_acc, c='r', label='Train Acc')\n",
    "    ax.plot(np.arange(n_epochs), test_acc, c='b', label='Test Acc')\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.set_ylim(0, 1.1)\n",
    "\n",
    "    final_accuracy = np.mean(test_acc[-10:])\n",
    "    final_accuracy = f\"Final Mean Test Accuracy: {100 * final_accuracy:.1f}%\"\n",
    "    ax.annotate(final_accuracy, xy=(0, 0.1),\n",
    "                textcoords=\"data\", fontsize=16)\n",
    "\n",
    "    plt.suptitle(title, fontsize=24)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0ub034MUYogz"
   },
   "outputs": [],
   "source": [
    "def run_experiment(**kwargs):\n",
    "    \"\"\"\n",
    "    Required kwargs:\n",
    "    - title:                A name for this experiment\n",
    "\n",
    "    - radii:                radii of rings in the multiple_circles dataset\n",
    "    - examples_per_ring:    number of examples per ring\n",
    "\n",
    "    - layer_sizes:          number of nodes per hidden layer in network\n",
    "    - activation:           activation function for each hidden layer\n",
    "    - learning_rate:        learning rate for SGD optimization\n",
    "    - n_pretrain_epochs:    how many epochs to pretrain\n",
    "    - n_train_epochs:       how many epochs to train\n",
    "    \"\"\"\n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    pretrain_data = make_multiple_circles(\n",
    "        radii=kwargs['radii'], examples_per_ring=kwargs['examples_per_ring']\n",
    "    )\n",
    "    pretrain_data = numpy_to_tensor(pretrain_data)\n",
    "\n",
    "    # Do not change the number of spiral examples!\n",
    "    spiral_data = make_spiral(32)\n",
    "    spiral_data = numpy_to_tensor(train_test_split(*spiral_data, test_size=0.2, random_state=1))\n",
    "\n",
    "    pretrained_model, trained_model, results = pretrain_and_train(\n",
    "        pretrain_data, spiral_data, **kwargs)\n",
    "\n",
    "    plot_results(pretrained_model, trained_model,\n",
    "                 pretrain_data, spiral_data,\n",
    "                 results, kwargs[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KoPk_4YiaZ9_",
    "outputId": "24c558d6-7ded-40b5-c3e9-3eb27ad4f56c"
   },
   "outputs": [],
   "source": [
    "# Example experiment; you should copy this cell several times,\n",
    "#   edit the kwargs, and save the figure outputs to your notebook.\n",
    "kwargs = {\n",
    "    \"title\": \"Example Experiment\",\n",
    "    \"radii\": (2, 6, 10),\n",
    "    \"examples_per_ring\": 100,\n",
    "    \"layer_sizes\": [100, 10],\n",
    "    \"activation\": torch.tanh,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"n_pretrain_epochs\": 100,\n",
    "    \"n_train_epochs\": 1000,\n",
    "}\n",
    "\n",
    "run_experiment(**kwargs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
